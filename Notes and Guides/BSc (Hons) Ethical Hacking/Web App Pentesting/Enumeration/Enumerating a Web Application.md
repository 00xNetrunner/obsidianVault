- [[#Robots.txt|Robots.txt]]
	- [[#Robots.txt#Curl|Curl]]
- [[#Identifying Technologies using tooling|Identifying Technologies using tooling]]
	- [[#Identifying Technologies using tooling#Passive Methods|Passive Methods]]
	- [[#Identifying Technologies using tooling#Active Methods|Active Methods]]
		- [[#Active Methods#Using WhatWeb|Using WhatWeb]]
- [[#NAVIGATING THE WEB LIKE A SPIDER|NAVIGATING THE WEB LIKE A SPIDER]]
	- [[#NAVIGATING THE WEB LIKE A SPIDER#MANUAL SPIDERING (With a proxy)|MANUAL SPIDERING (With a proxy)]]
	- [[#NAVIGATING THE WEB LIKE A SPIDER#AUTOMATED SPIDERING|AUTOMATED SPIDERING]]
	- [[#NAVIGATING THE WEB LIKE A SPIDER#Using OWASP ZAP|Using OWASP ZAP]]
##  Robots.txt
-----
Robots.txt can help hackers identify technologies used in the website, it can also help ackers identify folders that could be of particular interest. for example this robots.txt file lists a folder calls passwords. 
![[Pasted image 20240930192035.png]]

![[Pasted image 20240930192141.png]]

clicking the heroes.xml we will be presented with a list of users with the passwords visible 

![[Pasted image 20240930192250.png]]

### Curl
We can use the curl command on kali Linux or any other Linux distro to download the robots.txt and run it with grep to see folders with particular interest. 

```bash
curl -O 192.168.1.100/bWAPP/robots.txt
```

when downloaded we can then use the cat command to see the file. 

sometimes the robots.txt can have a lot of information in them, so we can use grep to just get the information we need. 

```bash
curl -s 192.168.1.100/bWAPP/robots.txt | grep "Disallow"
```

![[Pasted image 20240930193002.png]]

![[Pasted image 20240930193133.png]]

## Identifying Technologies using tooling
----
### Passive Methods
-----
these 3rd party tools will not work on intranet test.

if you use any of these tools and put a URL in them it will give you a list of all the technologies being used by this website. 

here is a list of sites that hackers will use for enumerating a website. 

- https://sitereport.netcraft.com/
- https://whatcms.org/
- https://www.wappalyzer.com/
- https://builtwith.com/

### Active Methods
-----
#### Using WhatWeb

```bash
whatweb 192.168.1.100
```

- the output for the command will look something like this.
![[Pasted image 20240930194132.png]]

## NAVIGATING THE WEB LIKE A SPIDER
------
### MANUAL SPIDERING (With a proxy)

![[Pasted image 20240930195413.png]]

### AUTOMATED SPIDERING 
-----
### Using OWASP ZAP

- from ozap select manual explore and input the target URL `192.168.1.100/WackoPicko/index.php`
![[Pasted image 20240930202601.png]]
- Then click launch browser. 

- when the browser opens up the webpage it will create a folder with the website. 
![[Pasted image 20240930202825.png]]

- right click on this folder Hover over Attack > Spider 

![[Pasted image 20240930203008.png]]
- make sure `Recurse` and `Spider Subtree Only` is selected and start the scan. 

![[Pasted image 20240930203242.png]]
- Now click the export button and save it. you can then view the file and it will show you all of the urls the spider found. 

- the file should look something like this
![[Pasted image 20240930203406.png]]

## IDENTIFYING HTTP HEADERS
-----
### USING SECURITY HEADERS


> [!NOTE] Security Headers Website
>  https://securityheaders.com/
>   Security Headers Website is an online tool that will scan the website and generate a report card

> [!tip] Take Note
> Make sure to click hide results to hide sensitive information

![[Pasted image 20241007124348.png]]

### USING NMAP
