##  Robots.txt
-----
Robots.txt can help hackers identify technologies used in the website, it can also help ackers identify folders that could be of particular interest. for example this robots.txt file lists a folder calls passwords. 
![[Pasted image 20240930192035.png]]

![[Pasted image 20240930192141.png]]

clicking the heroes.xml we will be presented with a list of users with the passwords visible 

![[Pasted image 20240930192250.png]]

### Curl
We can use the curl command on kali Linux or any other Linux distro to download the robots.txt and run it with grep to see folders with particular interest. 

```bash
curl -O 192.168.1.100/bWAPP/robots.txt
```

when downloaded we can then use the cat command to see the file. 

sometimes the robots.txt can have a lot of information in them, so we can use grep to just get the information we need. 

```bash
curl -s 192.168.1.100/bWAPP/robots.txt | grep "Disallow"
```

![[Pasted image 20240930193002.png]]

![[Pasted image 20240930193133.png]]

## Identifying Technologies using tooling
----
### Passive Methods
-----
these 3rd party tools will not work on intranet test.

if you use any of these tools and put a URL in them it will give you a list of all the technologies being used by this website. 

here is a list of sites that hackers will use for enumerating a website. 

- https://sitereport.netcraft.com/
- https://whatcms.org/
- https://www.wappalyzer.com/
- https://builtwith.com/

### Active Methods
-----
#### Using WhatWeb

```bash
whatweb 192.168.1.100
```

- the output for the command will look something like this.
![[Pasted image 20240930194132.png]]

## NAVIGATING THE WEB LIKE A SPIDER
------
### MANUAL SPIDERING (With a proxy)

![[Pasted image 20240930195413.png]]

### AUTOMATED SPIDERING 
-----
### Using OWASP ZAP

