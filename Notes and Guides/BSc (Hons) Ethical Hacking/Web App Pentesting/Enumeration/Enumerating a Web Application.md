```table-of-contents
title: 
style: nestedList # TOC style (nestedList|nestedOrderedList|inlineFirstLevel)
minLevel: 0 # Include headings from the specified level
maxLevel: 0 # Include headings up to the specified level
includeLinks: true # Make headings clickable
debugInConsole: false # Print debug info in Obsidian console
```
##  Robots.txt
-----
Robots.txt can help hackers identify technologies used in the website, it can also help ackers identify folders that could be of particular interest. for example this robots.txt file lists a folder calls passwords. 
![[Pasted image 20240930192035.png]]

![[Pasted image 20240930192141.png]]

clicking the heroes.xml we will be presented with a list of users with the passwords visible 

![[Pasted image 20240930192250.png]]

### Curl
We can use the curl command on kali Linux or any other Linux distro to download the robots.txt and run it with grep to see folders with particular interest. 

```bash
curl -O 192.168.1.100/bWAPP/robots.txt
```

when downloaded we can then use the cat command to see the file. 

sometimes the robots.txt can have a lot of information in them, so we can use grep to just get the information we need. 

```bash
curl -s 192.168.1.100/bWAPP/robots.txt | grep "Disallow"
```

![[Pasted image 20240930193002.png]]

![[Pasted image 20240930193133.png]]

## Identifying Technologies using tooling
----
### Passive Methods
-----
these 3rd party tools will not work on intranet test.

if you use any of these tools and put a URL in them it will give you a list of all the technologies being used by this website. 

here is a list of sites that hackers will use for enumerating a website. 

- https://sitereport.netcraft.com/
- https://whatcms.org/
- https://www.wappalyzer.com/
- https://builtwith.com/

### Active Methods
-----
#### Using WhatWeb

```bash
whatweb 192.168.1.100
```

- the output for the command will look something like this.
![[Pasted image 20240930194132.png]]

## NAVIGATING THE WEB LIKE A SPIDER
------
### MANUAL SPIDERING (With a proxy)

![[Pasted image 20240930195413.png]]

### AUTOMATED SPIDERING 
-----
### Using OWASP ZAP

- from ozap select manual explore and input the target URL `192.168.1.100/WackoPicko/index.php`
![[Pasted image 20240930202601.png]]
- Then click launch browser. 

- when the browser opens up the webpage it will create a folder with the website. 
![[Pasted image 20240930202825.png]]

- right click on this folder Hover over Attack > Spider 

![[Pasted image 20240930203008.png]]
- make sure `Recurse` and `Spider Subtree Only` is selected and start the scan. 

![[Pasted image 20240930203242.png]]
- Now click the export button and save it. you can then view the file and it will show you all of the urls the spider found. 

- the file should look something like this
![[Pasted image 20240930203406.png]]

## IDENTIFYING HTTP HEADERS
-----
### USING SECURITY HEADERS


> [!NOTE] Security Headers Website
>  https://securityheaders.com/
>   Security Headers Website is an online tool that will scan the website and generate a report card

> [!tip] Take Note
> Make sure to click hide results to hide sensitive information

![[Pasted image 20241007124348.png]]

### USING NMAP

- From a terminal in Kali, run the following commands against the target. 
```bash
nmap 192.168.1.100 -p80,443 --script=http-headers
nmap 192.168.1.100 -p80,443 --script=http-methods
```

results will look like this: 
![[Pasted image 20241007130752.png]]

- we can copy this information and put it into chatGPT or pentestgpt for analysis, off the bat though we can see a lot of outdated software
![[Pasted image 20241007131001.png]]

## IDENTIFYING JAVASCRIPT FILES AND LINKS
------------

Most modern web applications make use of client-side JavaScript. alot of programmers hardcode sensitive information in JavaScript, information can include Private API Keys, Internal IP addresses, sensitive routes, and sometimes credentials. hackers can even find expoitable code and discover outdated frameworks to find .js files we are going to be using hakrawler 

```bash
echo http://127.0.0.1:3000/#/ | hakrawler
```
- we are running it against juiceshop located at 127.0.0.1:3000 and examine the output

![[Pasted image 20241007142842.png]]
- if we click on any of these links we will get something like this
  ![[Pasted image 20241007142925.png]]
> [!NOTE] Deobfuscator Tool
> We can use tools like http://deobfuscate.io/ to deobfuscate the code. we can even use tools like pentestGPT to point out vulerable information and code

![[Pasted image 20241007143347.png]]
- list of vulns found in the code. 

## BRUTE-FORCING HIDDEN FOLDERS AND FILES
-----
### Using dirb
one of the most common bruteforcing tool is dirb, drib will find hidden pages on a website like an admin login page for example. 

- From kali terminal input the following
```bash
dirb http://192.168.1.100/WackoPicko
```

![[Pasted image 20241007172027.png]]

dirb can also use wordlists to use a wordlist enter the following command
```bash
dirb http://192.168.1.100/WackoPicko /usr/share/dirb/wordlists/common.txt
```

> [!NOTE] Take note!
> These wordlists or Dictionaries can be found in `/usr/share/dirb/wordlists` & `/usr/share/dirbuster/wordlists`

### dirbuster
Another tool we can use is dirbuster to use this input `dirbuster` into the terminal this will launch the GUI 
![[Pasted image 20241007172443.png]]

### gobuster

gobuster works similar to dirb and can be used 